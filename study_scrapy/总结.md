学习：https://www.bilibili.com/video/BV1Db4y1m7Ho?p=104

1. 安装 pip install wheel 和 pip install pyOpenSSL

2. 安装 pip install scrapy
参考：https://www.runoob.com/note/37641

3. 【创建项目】项目目录下：D:\Python_Study\爬虫\study_scrapy>scrapy startproject study_scrapy
比如：scrapy startproject 项目名

4. 【创建爬虫文件】D:\Python_Study\爬虫\study_scrapy\demo\demo>scrapy genspider baidu www.baidu.com
比如：scrapy genspider 子项目名 要爬取的url

5. 运行爬虫文件：scrapy crawl baidu


链接提取器使用：readbook总结

1. 创建项目

2. cd 项目名/项目名/spiders

3. 创建爬虫文件
scrapy genspider -t crawl 爬虫文件的名字 爬取的域名

cmd使用链接提取器：

1. scrapy shell https://www.dushu.com/book/1617.html

2. 导入所需要的包：
from scrapy.linkextractors import LinkExtractor

3. link = LinkExtractor
   # 定义爬取正则表达式
   link = LinkExtractor(allow=r'book/1617_\d+.html')

4. 获取爬取的数据
link.extract_links(response)

日志

# 指定日志的级别
# CRITICAL:严重错误，ERROR：一般错误，WARNING：警告，
# INFO：一般信息，DEBUG：调试信息（默认级别）
LOG_LEVEL = 'INFO'
# 日志文件：记录屏幕显示的全部记录，到spiders下
LOG_FILE = 'logo.log'
